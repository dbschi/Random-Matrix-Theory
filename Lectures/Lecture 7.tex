\section{Applications of Universality}

We first introduce a couple examples, for which the connect to random matrices is not very clear. (But I promise they are related)

\begin{aexample}{Totally Asymmetric Simple Exclusion Process/ TASEP}{}
    We model a semi infinite line of electrons (or any particle). At $t=0$, all the electrons occupy the non-negative integer spaces. Particle $x_k$ is at $-k$, $k\geq 0$.

    \begin{center}
        
    \begin{tikzpicture}[scale=1]
    % Draw the number line
    \draw[->] (-4.5,0) -- (4.5,0);
    \draw[red,thick,dotted](0.5,0.5)node[above]{Detector} -- (0.5,-0.5);
    % Draw filled dots for non-positive integers
    \foreach \x in {-4,...,0} {
        \filldraw[black] (\x,0) circle (2pt);
        \node[below] at (\x,-0.2) {\x};
    }
    
    % Draw hollow dots for positive integers
    \foreach \x in {1,...,4} {
        \draw[black] (\x,0) circle (2pt);
        \node[below] at (\x,-0.2) {\x};
    }
\end{tikzpicture}
    \end{center}

    A strong positive charge at $+\infty$ attracts the electrons independently and identically. Formally, each particle $x_k$ has an internal clock $T_k$ with distribution \[
    \mathbb{P}(T_k> s) = f(s).
    \]
    When the clock rings for $x_k$, $x_k$ will move to the right by one unit, provided that the space in front is empty (not occupied by another electron). The clock then resets to $0$ and $T_k$ is recounted again for the next time the particle moves.
    
    There is a current detector between $0$ and $1$. We want to know the following:
    
    \begin{quote}
        Given time $t>0$, what is the number of particles that passed the current detector? I.e. \[
        y_t \defeq \#\{i:x_i(t)>0\}
        \]
    \end{quote}

\end{aexample}

This was an open question for around 40 years. In the 1940's it was conjectured that \[
y_t \sim ct
\]
for a constant $c$. This was solved in 1988' by Ulm, for the case $\mathbb{P}(T_k\geq s) = e^{-s}$ which is the exponential distribution. In fact, we only understand the cases for $T_k$ is exponentially distributed, or when $T_k$ is geometrically distributed - the former being the limit of the latter. The asymptotics for general $T_k$ is widely open.

\begin{remark}
    I have heard stories about a direct competitor of my firm (one that eats into our profits) giving out this question in an interview with $T_k \sim \text{Geom}(1/2)$. Of course they made it look simple by changing the wording to `decided by a coin flip', like most quant firms. Not sure what they were looking for giving out this question to propsective intern candidates, or maybe I misunderstood and that this question was not even asked - I've never interviewed with them after all.
\end{remark}

\begin{aexample}[breakable=false]{Last Passage Percolation/ LPP}{}
    
\begin{center}
    
    \begin{tikzpicture}[scale=0.8]
    % Draw the grid
    \foreach \x in {0,...,4} {
        \draw[gray!30] (\x,0) -- (\x,4);
    }
    \foreach \y in {0,...,4} {
        \draw[gray!30] (0,\y) -- (4,\y);
    }
    
    % Draw axes with arrows
    \draw[->] (0,0) -- (4.5,0) node[right] {$x$};
    \draw[->] (0,0) -- (0,4.5) node[above] {$y$};
    
    % Draw grid points
    \foreach \x in {0,...,4} {
        \foreach \y in {0,...,4} {
            \filldraw[black] (\x,\y) circle (1.5pt);
        }
    }
    
    % Draw a sample staircase (3,)
    \draw[red, thick] (0,0) -- (0,1) -- (1,1) -- (1,2) -- (2,2) -- (2,3) -- (3,3) -- (3,4)--(4,4);
    
    % Label start and end points
    \node[below left] at (0,0) {$(0,0)$};
    \node[right] at (4,4) {$(4,4)$};
\end{tikzpicture}

\end{center}

We now move to a 2-dimensional example. Let us cover all lattice points $v=(x\geq 0,y\geq 0)$ with an IID variable $w_v$. Fix $X,Y$ For each up-right path $\gamma$ from $(0,0) \to (X,Y)$, we let $T(\gamma)\defeq \sum_{v\in \gamma} w_v$.
What are the asymptotics of \[
L(x,y) = \max_{\gamma \text{ from } (0,0)\to (x,y)} T(\gamma)
\]
as $(x,y)\to (\infty,\infty)$, along $x=y$?

\end{aexample}
\begin{remark}
    You can read \href{https://arxiv.org/pdf/math/9903134}{Johansson} for details on this. (Later, because this spoils the fun)
\end{remark}
\begin{aexample}{Longest Increasing Subsequence}{}
    Fix N. Let $\sigma$ be a permutation of $\{1,...,N\}$ drawn uniformly in $S_N$, and $L_N$ be the longest increasing subsequence of $\sigma$. What is the asymptotic behaviour of $L_N$?  
\end{aexample}

\subsection*{TASEP is LLP}
The punchline is that these two problems are the same for exponential and geometric random variables. The first observation is that exponential and geometric random variables are memoryless i.e. \[
\mathbb{P}(T_k>n+m\ |\ T_k>m) = \mathbb{P}(T_k>n).
\]
This means we can view the internal clock in TASEP as a different clock (with the same distribution) that starts counting only if the space in front of it is empty. I will leave you to verify the details: there should be a change in the measure, but the distribution of the observable states does not change.

Now we give a mapping between $w_v$ in LLP to the states in TASEP. We imagine each particle $x_k$ lives on the line $y=k$, and they are moving to the right. The transformation is: \begin{quote}
    $w_{x,y}$ describes the time needed for particle $y$ to make the $x+1$-st step after the opportunity shows up.
\end{quote}
For instance, suppose particle $x_3$ (initially at $-3$) is now at $2$. It has taken $5$ steps already. The moment $3$ shows up empty, it takes $w_{5,3}$ time for it to move to the empty spot.


The condition for $n$ particles to pass the detector is equivalent to $x_{n-1}$ moving forward $n$ spots. So given all the internal states $w_{x,y}$, we can consider a dynamic programming approach to TASEP: Let $a_{x,y}$ describe the time needed for particle $y$ to move forward $x+1$ steps (the indexing is done to align with $w_{x,y}$). Then the base case is that \[
a_{x,y}=0
\]
if $x<0$ or $y<0$. We want particle to move $0$ steps.
The recurrance relation is
\[
a_{x,y} = \max(\underbrace{a_{x-1,y}}_{\text{particle $y$ is in previous spot}}, \underbrace{a_{x,y-1}}_{\text{next spot opens up}})+\underbrace{w_{x,y}}_{\text{particle $y$ moves}} .
\] 
This is exactly the recurrance relation for the longest path from $(0,0)$ to $(x,y)$, which is LLP!

\proposition[]{
    For geometric/exponentially distributed random variables in TASEP / LLP,

    \[
    \mathbb{P}(y_t > n)=\mathbb{P}(L(n-1,n-1)<t). 
    \]

}
\subsection*{More on the transformation}
For each state $S$ in TASEP, we can represent it as a function $f_S$ with the property \[
f_S (x) = \begin{cases*}
    f_S(x-1)+1, & \text{if }x \text{ is not occupied by a particle},\\
    f_S(x-1)-1, & \text{if }x \text{ is occupied by a particle}.
\end{cases*}
\]
This function is defined up to an arbitrary constant, so let us define function corresponding to the initial state as $|x|$.
\begin{center}\begin{tikzpicture}[scale=1]
    \draw[->] (-4.5,0) -- (4.5,0) node[right] {$x$};
    \draw[->] (0,-0.5) -- (0,4.5) node[above] {$y$};
    
    \draw[blue, thick] (-4,4) -- (0,0) -- (4,4);
    
    \foreach \x in {-4,...,0} {
        \filldraw[black] (\x,0) circle (2pt);
        \node[below] at (\x,-0.2) {\x};
    }
    
    \foreach \x in {1,...,4} {
        \draw[black] (\x,0) circle (2pt);
        \node[below] at (\x,-0.2) {\x};
    }
    
    % Label the function
    %\node[above left] at (-2,2) {$|x|$};
\end{tikzpicture}
\end{center}
When $x_0$ moves one step to the right, what happens to the graph of the state function? We would expect most of the function to be the same, except $f(0)$ and $f(1)$; those are the two places that changed `vacancy status'. Indeed, we can define the new state function as 
\begin{center}\begin{tikzpicture}[scale=1]
    \draw[->] (-4.5,0) -- (4.5,0) node[right] {$x$};
    \draw[->] (0,-0.5) -- (0,4.5) node[above] {$y$};
    
    \draw[blue, thick] (-4,4) --(-1,1)--(0,2)-- (1,1) -- (4,4);
    
    \foreach \x in {-4,...,-1,1} {
        \filldraw[black] (\x,0) circle (2pt);
        \node[below] at (\x,-0.2) {\x};
    }
    
    \foreach \x in {0,2,3,4} {
        \draw[black] (\x,0) circle (2pt);
        \node[below] at (\x,-0.2) {\x};
    }
    
    % Label the function
    %\node[above left] at (-2,2) {$|x|$};
\end{tikzpicture}
\end{center} 

This is a common way to \href{https://www.youtube.com/watch?v=QXBSxmL_Ric}{visualize TASEP}. 
We now rotate this graph 45 degrees (and rescale by $\sqrt{2}$) and superimpose it onto the LLP setup.


\begin{center}
    
    \begin{tikzpicture}[scale=0.8]
    % Draw the grid
    \foreach \x in {0,...,4} {
        \draw[gray!30] (\x,0) -- (\x,4);
    }
    \foreach \y in {0,...,4} {
        \draw[gray!30] (0,\y) -- (4,\y);
    }
    
    % Draw axes with arrows
    \draw[->] (0,0) -- (4.5,0) node[right] {$x$};
    \draw[->] (0,0) -- (0,4.5) node[above] {$y$};
    
    % Draw grid points
    \foreach \x in {0,...,4} {
        \foreach \y in {0,...,4} {
            \filldraw[black] (\x,\y) circle (1pt);
        }
    }

    \draw[blue, thick] (0,4) --(0,1)--(1,1)-- (1,0) -- (4,0);
    \foreach \x in {-4,...,4} {
        \draw[dashed,thin] (\x/2,-\x/2) -- ({4+\x/2},4-\x/2);
    }
    \foreach \x in {-4,...,-1,1} {
        \filldraw[black] (\x/2,-\x/2
) circle (2pt);
        \node[below left] at (\x/2
,-\x/2
-0.2) {\x};
    }
    
    \foreach \x in {0,2,3,4} {
        \draw[black] (\x/2
,-\x/2
) circle (2pt);
        \node[below left] at (\x/2
,-\x/2
-0.2) {\x};
    }
    
\end{tikzpicture}

\end{center}

\example[]{
    Try to convince yourself for every state, the distribution of the time it takes for the $n$-th particle to move to $m-n$ is the maximum up-right path starting from any point in the corresponding blue curve to the point $(m-1,n)$.
}


Our approach is to solving LLP is this\footnote{This is not the approach by Ulm.}: We first solve for geometric random variables then extend the result to exponential random variables as a limit of geometric.

Let $w_v \sim \text{Geom}(q)$, $0< q< 1$. That is \[
\mathbb{P}(w_v=k) = (1-q)^{k} q.
\]
Then we would have for $q=1-1/L$ as $L\to \infty$, $w_v/L \to \exp(1)$.

The value of $L(N-1,N-1)$ only depends on the $N^2$ vertices in the square $(0,0)$ -- $(N-1,N-1)$. For ease of notation, we index everything from $1$ instead of $0$ to have a matrix indexed from $1$ to $N$. We can represent this as a random matrix \[
M_{x,y} = (w_{(x,y)})
\]
with integer entries. The distribution of this matrix is given by \[
\mathbb{P}(M=A) = \prod_{i,j\leq N} \mathbb{P}(M_{i,j} = a_{i,j}) = (1-q)^{\sum a_{i,j}} q^{N^2}.
\]

This means that the frequency of each matrix only depends on the sum of the entries. Conditioned on the sum of the entries, the distribution is actually uniform over all possible matrices! So we have \begin{align*}
    \mathbb{P}(L(N-1,N-1)\leq t) =& \sum_{k=0}^{\infty} \mathbb{P}\Big(L(N-1,N-1) \ |\ \sum m_{i,j}=k\Big)\mathbb{P}\Big(\sum m_{i,j}=k\Big)\\
    =& \sum_{k=0}^{\infty} \frac{\#\overbrace{ \{L(N-1,N-1)\leq t \ |\ \sum m_{i,j}=k\}}^{\text{this is the hard part}}}{\#\underbrace{\{ \sum m_{i,j}=k\}}_{\text{not related to LPP}}} \underbrace{\mathbb{P}\Big(\sum m_{i,j}=k\Big)}_{\text{not related to LLP}}.
\end{align*}

\subsection*{LLP is Longest Increasing Subsequence}
We now translate the counting problem into longest increasing subsequence. This is a slightly different problem in that we are finding longest increasing subsequences in what is known as \textbf{generalized permutations}. 
\definition{Generalized Permutation}{
    Let $A$ be a matrix with non-negative integer entries. The \textbf{generalized permutation} associated with $A$ is defined as the two-line array \[
    \sigma_A \defeq \left(\begin{array}{ccccccccc}
        1 & 1 & ... & 1 & 1 & ... & 1&...&N\\
        \undermat{a_{1,1} \text{ times}}{1 & 1 & ... & 1} & \undermat{a_{1,2} \text{ times}}{2 & ... & 2} &...&N\\
    \end{array}\right).
    \] 
    \\
    
    
}
\begin{aexample}{}{}
    Let $A$ be a permutation matrix. Then $\sigma_A$ is in the form \[
    \begin{bmatrix}
        1 & 2 &... & N\\
        \sigma(1) & \sigma(2) & ... &\sigma(N)
    \end{bmatrix}
    \]
    which is the standard two-line representation of a permutation.
\end{aexample}

\proposition{
    Given the matrix $M$, $L(N-1,N-1)$ is the length of the longest increasing subsequence of $\sigma_{M^T}$.
}
\begin{proof}[Idea of proof]
    For every path in $\gamma$ from $(0,0)$ to $(N-1,N-1)$, associate to an increasing subseqeunce in $\sigma_{M^T}$ (Hint: just take all the indices the path visited), and the length of the subsequence is the sum of weights. For every increasing subseqeunce, we can extend it to a longer subsequence (for each index $i,j$, we can take all of the columns or none of the columns) and it corresponds to a path with sum of weights equal to the length of the extended subseqeunce.
\end{proof}
\begin{remark}
    The length of the permutation is equal to the sum of entries of the matrix. So conditioned on the sum of the entries, the distribution of the generalized permutation is actually uniform.
\end{remark}
We thus solve two problems at once. The first one is in the context of LLP, and we calculate the longest increase subsequence of generalized permutations. The second is when the matrix in LLP is drawn uniformly from the distribution of permutation matrices, in which case we are looking for average longest increasing subsequence of a random permutation.

Like the trees in Wigner matrices, we have translated a problem in random matrices to a problem in combinatorics.

\definition{Young Diagram / Tableau}{
    A Young Diagram of shape $(\lambda_1,...,\lambda_n)$, $\lambda_i \geq \lambda{i+1}$ is a collection of boxes aligned on the left, with the $i$-th row having $\lambda_i$ columns. The size of a Young Diagram is the $\sum_i \lambda_i$.
    
    A standard Young Tableau is a filled in Young Diagram where the values are strictly increasing in each row and column. A semistandard Young tableau is a one that is strictly increasing in each column and increasing in each row.
    \begin{center}
        
   \begin{tikzpicture}[scale=0.8]
    % Draw the boxes
    \draw (0,3) -- (4,3); % top line
    \draw (0,2) -- (4,2); % second line
    \draw (0,1) -- (3,1); % third line
    \draw (0,0) -- (1,0); % bottom line
    
    % Vertical lines
    \draw (0,0) -- (0,3);
    \draw (1,0) -- (1,3);
    \draw (2,1) -- (2,3);
    \draw (3,1) -- (3,3);
    \draw (4,2) -- (4,3);
    
    % Fill with numbers (increasing in rows and columns)
    \node at (0.5,2.5) {1};
    \node at (1.5,2.5) {2};
    \node at (2.5,2.5) {3};
    \node at (3.5,2.5) {4};
    \node at (0.5,1.5) {5};
    \node at (1.5,1.5) {6};
    \node at (2.5,1.5) {7};
    \node at (0.5,0.5) {8};
    
\end{tikzpicture}

    \end{center}
}

\theorem[]{Robinson-Schensted-Knuth (RSK) Correspondence}{
    There is a bijection between generalized permutations of length $k$ and pairs of semi-standard Young Tableaux $\{(P,Q) \ |\  P,Q\text{ equal shape and of size } k\}$.

    Moreover, under this correspondence, there is a bijection between permutations of length $k$ and pairs of standard Young Tableaux of equal shape and size $k$.
}

We first describe going from permutation to $(P,Q)$. It involves inserting each column into $P$ and $Q$ one by one. 
\SetKw{kwin}{in}
\begin{algorithm}
    \caption{RSK Correspondence}
    \DontPrintSemicolon
    \KwData{A generalized permutation $\sigma = \{(a_i,b_i)\}$}
    \KwResult{Two Young tableaux $(P,Q)$ of the same shape}
    $P \gets$ empty tableau\;
    $Q \gets$ empty tableau\;
    \For{$(a_i,b_i)$ \kwin $\sigma$}{
        $RowNumber \gets 1$\;
        $x \gets b_i$\;  % Current number to insert
        \While{true}{
            \eIf{$\lambda_{RowNumber}=0$ \textbf{ or } $x > $ last element in row $RowNumber$}{
                Add $x$ to end of row $RowNumber$ in $P$\;
                Add $a_i$ to same position in $Q$\;
                \textbf{break}\;
            }{
                Find first element $y$ in row $RowNumber$ larger than $x$\;
                Swap $x$ and $y$\;  % $x$ now holds the bumped number
                $RowNumber \gets RowNumber + 1$\;
            }
        }
    }
\end{algorithm}
The inverse operation from $(P,Q)$ to a generalized permutation involves finding last inserted element using $Q$, then reverse the swapping logic until an element of the first row is obtained.

\proposition[]{
    Under this correspondence: \begin{enumerate}
        \item The length of the first row is the longest increasing subsequence of the permutation.
        \item The length of the first column is the longest descreasing subsequence of the permutation.
    \end{enumerate}
}
\begin{proof}
    Notice the evolution of the first row in $P$ describes exactly the process of a dynamic approach to the longest increasing subsequence with the state definition $i$-th element is the smallest last element among all increasing subsequences of length $i$.

    The evolution of the first column is the reverse: The $i$-th element is the smallest first element among all decreasing subequences of length $i$. 
\end{proof}
\begin{acorollary}{Erd\"os-Szekeres theorem}{}
    Any permutation of length $(a-1)(b-1)+1$ contains at least an increasing subsequence of length $a$ or a decreasing subseqeunce of length $b$.
\end{acorollary}
\begin{proof}
    Apply the Pigeonhole Theorem to the shape of the Tableau from the RSK correspondence.
    
\end{proof}

\begin{alemma}{}{}
    The number of semi-standard young tableaux of shape $(\lambda_i)$ with entries from $1$ to $N$ is \[
    \prod_{i< j}^{} \frac{\lambda_i -\lambda_j + j-i}{j-i}
    \]
\end{alemma}
\begin{proof}
    This is lemma 2.3 in Johansson.
\end{proof}

